{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec38efe-c508-4934-acca-766ca9cf0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fafc6-eea9-4282-9f67-14be46ddf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2024, 2019, -1))\n",
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "visited_urls = set()\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Fetching data for season {year}...\")\n",
    "    try:\n",
    "        # Fetch the standings page\n",
    "        data = requests.get(standings_url)\n",
    "        if standings_url in visited_urls:\n",
    "            print(\"Repeated URL detected. Stopping the loop.\")\n",
    "            break\n",
    "        visited_urls.add(standings_url)\n",
    "\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "        standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "        # Extract team URLs\n",
    "        links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "        links = [l for l in links if '/squads/' in l]\n",
    "        team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "\n",
    "        # Get the previous season URL\n",
    "        prev_links = soup.select(\"a.prev\")\n",
    "        if not prev_links:\n",
    "            print(\"No previous season link found. Stopping the loop.\")\n",
    "            break\n",
    "        previous_season = prev_links[0].get(\"href\")\n",
    "        standings_url = f\"https://fbref.com{previous_season}\"\n",
    "\n",
    "        # Loop through each team and get data\n",
    "        for team_url in team_urls:\n",
    "            team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "            print(f\"Fetching data for {team_name}...\")\n",
    "            try:\n",
    "                data = requests.get(team_url)\n",
    "                matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "                soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "                # Get shooting stats\n",
    "                links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "                links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "                if not links:\n",
    "                    print(f\"No shooting data found for {team_name}\")\n",
    "                    continue\n",
    "\n",
    "                shooting = pd.read_html(f\"https://fbref.com{links[0]}\", match=\"Shooting\")[0]\n",
    "                shooting.columns = shooting.columns.droplevel()\n",
    "\n",
    "                # Merge matches and shooting data\n",
    "                team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\", how='left')\n",
    "                team_data = team_data[team_data[\"Comp\"] == \"Premier League\"]\n",
    "                team_data[\"Season\"] = year\n",
    "                team_data[\"Team\"] = team_name\n",
    "                all_matches.append(team_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {team_name}: {e}\")\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching standings for season {year}: {e}\")\n",
    "        break\n",
    "\n",
    "# Combine all data\n",
    "if all_matches:\n",
    "    final_df = pd.concat(all_matches, ignore_index=True)\n",
    "    print(\"Data fetching completed successfully!\")\n",
    "else:\n",
    "    print(\"No data fetched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9cf72-21ab-45cc-8b2b-92a0f8051132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(team_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ba138-13ee-4ddf-81e8-41405aab4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttable = pd.concat(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7895a0f-a05e-4d7a-a6b4-290fce1d1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttable['GF'] = ttable['GF'].fillna(0).astype(int)\n",
    "ttable['GA'] = ttable['GA'].fillna(0).astype(int)\n",
    "ttable=ttable.reset_index()\n",
    "ttable.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e6109-077c-4c06-90f2-56ceb68d1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttable.to_csv('ttable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f7f88-db54-4f39-9d02-cfaaea2dec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2024, 2019, -1))\n",
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "\n",
    "for year in years:\n",
    "    data = requests.get(standings_url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    standings_table = soup.select('table.stats_table')\n",
    "\n",
    "    if not standings_table:\n",
    "        print(f\"No league table found for {standings_url}\")\n",
    "        continue\n",
    "    \n",
    "    standings_table = standings_table[0]\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "    links = [l for l in links if '/squads/' in l]\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")\n",
    "    if previous_season:\n",
    "        standings_url = f\"https://fbref.com{previous_season[0].get('href')}\"\n",
    "    else:\n",
    "        print(f\"No previous season link found for {standings_url}\")\n",
    "        break\n",
    "    \n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        data = requests.get(team_url)\n",
    "        \n",
    "        \n",
    "        matches = pd.read_html(StringIO(data.text), match=\"Scores & Fixtures\")[0]\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/passing/' in l]\n",
    "        \n",
    "        if not links:\n",
    "            print(f\"No passing data link found for {team_name} in {year}\")\n",
    "            continue\n",
    "        \n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        passing = pd.read_html(StringIO(data.text), match=\"Passing\")[0]\n",
    "        passing.columns = passing.columns.droplevel()\n",
    "        \n",
    "       \n",
    "        try:\n",
    "            team_data1 = matches.merge(\n",
    "                passing[[\"Date\", \"KP\", \"1/3\", \"PPA\", \"CrsPA\", \"PrgP\"]], \n",
    "                on=\"Date\", how=\"left\"\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError for merging data of {team_name} in {year}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        team_data1 = team_data1[team_data1[\"Comp\"] == \"Premier League\"]\n",
    "        team_data1[\"Season\"] = year\n",
    "        team_data1[\"Team\"] = team_name\n",
    "        all_matches.append(team_data1)\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "if all_matches:\n",
    "    ttable = pd.concat(all_matches, ignore_index=True)\n",
    "    print(f\"Collected data for {len(ttable)} matches.\")\n",
    "else:\n",
    "    print(\"No matches collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b5305-c02e-486e-ae4f-0b082f1fcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "assists = pd.concat(all_matches)\n",
    "assists=assists.reset_index()\n",
    "assists.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5fa5f-1479-4f92-864f-7d1add210a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "assists.to_csv('assists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f11bff-430a-4a53-b4d9-8271077caab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2024, 2019, -1))\n",
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "\n",
    "for year in years:\n",
    "    data = requests.get(standings_url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    standings_table = soup.select('table.stats_table')\n",
    "\n",
    "    if not standings_table:\n",
    "        print(f\"No league table found for {standings_url}\")\n",
    "        continue\n",
    "    \n",
    "    standings_table = standings_table[0]\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "    links = [l for l in links if '/squads/' in l]\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")\n",
    "    if previous_season:\n",
    "        standings_url = f\"https://fbref.com{previous_season[0].get('href')}\"\n",
    "    else:\n",
    "        print(f\"No previous season link found for {standings_url}\")\n",
    "        break\n",
    "    \n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        data = requests.get(team_url)\n",
    "        \n",
    "        \n",
    "        matches = pd.read_html(StringIO(data.text), match=\"Scores & Fixtures\")[0]\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/possession/' in l]\n",
    "        \n",
    "        if not links:\n",
    "            print(f\"No possession data link found for {team_name} in {year}\")\n",
    "            continue\n",
    "        \n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        possession = pd.read_html(StringIO(data.text), match=\"Possession\")[0]\n",
    "        possession.columns = possession.columns.droplevel()\n",
    "\n",
    "        \n",
    "        required_columns = [\"Att 3rd\", \"Att Pen\", \"PrgC\"]\n",
    "        available_columns = [col for col in required_columns if col in possession.columns]\n",
    "\n",
    "        if not available_columns:\n",
    "            print(f\"Required possession columns not found for {team_name} in {year}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            \n",
    "            team_data2 = matches.merge(possession[[\"Date\"] + available_columns], on=\"Date\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Merge error for {team_name} in {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        team_data2 = team_data2[team_data2[\"Comp\"] == \"Premier League\"]\n",
    "        team_data2[\"Season\"] = year\n",
    "        team_data2[\"Team\"] = team_name\n",
    "        all_matches.append(team_data2)\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "if all_matches:\n",
    "    ttable = pd.concat(all_matches, ignore_index=True)\n",
    "    print(f\"Collected data for {len(ttable)} matches.\")\n",
    "else:\n",
    "    print(\"No matches collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b513b-2372-47b0-a9f7-e878b6eed261",
   "metadata": {},
   "outputs": [],
   "source": [
    "posses = pd.concat(all_matches)\n",
    "posses=posses.reset_index()\n",
    "posses.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ae70c-8552-4ec8-925f-6171dcc90e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posses.to_csv('possession.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c7e72-a8f1-4156-9fe2-41b58c0d572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2024, 2019, -1))\n",
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "\n",
    "for year in years:\n",
    "    data = requests.get(standings_url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    standings_table = soup.select('table.stats_table')\n",
    "\n",
    "    if not standings_table:\n",
    "        print(f\"No league table found for {standings_url}\")\n",
    "        continue\n",
    "    \n",
    "    standings_table = standings_table[0]\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "    links = [l for l in links if '/squads/' in l]\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")\n",
    "    if previous_season:\n",
    "        standings_url = f\"https://fbref.com{previous_season[0].get('href')}\"\n",
    "    else:\n",
    "        print(f\"No previous season link found for {standings_url}\")\n",
    "        break\n",
    "    \n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        data = requests.get(team_url)\n",
    "        \n",
    "        \n",
    "        matches = pd.read_html(StringIO(data.text), match=\"Scores & Fixtures\")[0]\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/gca/' in l]\n",
    "        \n",
    "        if not links:\n",
    "            print(f\"No goal and shot creation data link found for {team_name} in {year}\")\n",
    "            continue\n",
    "        \n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        gfc = pd.read_html(StringIO(data.text), match=\"Goal and Shot Creation\")[0]\n",
    "        gfc.columns = gfc.columns.droplevel()\n",
    "\n",
    "       \n",
    "        if \"Date\" not in gfc.columns or \"SCA\" not in gfc.columns:\n",
    "            print(f\"Required columns not found for {team_name} in {year}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            \n",
    "            team_data3 = matches.merge(gfc[[\"Date\", \"SCA\"]], on=\"Date\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Merge error for {team_name} in {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        team_data3 = team_data3[team_data3[\"Comp\"] == \"Premier League\"]    \n",
    "        team_data3[\"Season\"] = year\n",
    "        team_data3[\"Team\"] = team_name\n",
    "        all_matches.append(team_data3)\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "if all_matches:\n",
    "    ttable = pd.concat(all_matches, ignore_index=True)\n",
    "    print(f\"Collected data for {len(ttable)} matches.\")\n",
    "else:\n",
    "    print(\"No matches collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f877b-4b6b-4393-bb8b-2a4579f20077",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation = pd.concat(all_matches)\n",
    "creation=creation.reset_index()\n",
    "creation.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536c12f-e3e8-460b-8d9c-0b363df5c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation.to_csv('creation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96873b-8d22-47e5-b2cd-a37d8a1d2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2024, 2019, -1))\n",
    "all_matches = []\n",
    "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "\n",
    "for year in years:\n",
    "    data = requests.get(standings_url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    standings_table = soup.select('table.stats_table')\n",
    "    if not standings_table:\n",
    "        print(f\"No table found for {standings_url}\")\n",
    "        continue\n",
    "    \n",
    "    standings_table = standings_table[0]\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "    links = [l for l in links if '/squads/' in l]\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")\n",
    "    if not previous_season:\n",
    "        print(f\"No previous season link found for {standings_url}\")\n",
    "        break\n",
    "    \n",
    "    standings_url = f\"https://fbref.com{previous_season[0].get('href')}\"\n",
    "    \n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        data = requests.get(team_url)\n",
    "        \n",
    "        \n",
    "        matches = pd.read_html(StringIO(data.text), match=\"Scores & Fixtures\")[0]\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/passing_types/' in l]\n",
    "        \n",
    "        if not links:\n",
    "            print(f\"No passing type link found for {team_name} in {year}\")\n",
    "            continue\n",
    "        \n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        passing_type = pd.read_html(StringIO(data.text), match=\"Pass Types\")[0]\n",
    "        passing_type.columns = passing_type.columns.droplevel()\n",
    "        \n",
    "        \n",
    "        if \"Date\" not in passing_type.columns or \"TB\" not in passing_type.columns:\n",
    "            print(f\"Required columns not found for {team_name} in {year}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            team_data5 = matches.merge(passing_type[[\"Date\", \"TB\"]], on=\"Date\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Merge error for {team_name} in {year}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        team_data5 = team_data5[team_data5[\"Comp\"] == \"Premier League\"]\n",
    "        team_data5[\"Season\"] = year\n",
    "        team_data5[\"Team\"] = team_name\n",
    "        all_matches.append(team_data5)\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "if all_matches:\n",
    "    full_table = pd.concat(all_matches, ignore_index=True)\n",
    "    print(f\"Collected data for {len(full_table)} matches.\")\n",
    "else:\n",
    "    print(\"No matches collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58ea0f-8506-47a3-b915-57fc68ca50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "back1=pd.concat(all_matches)\n",
    "back1=back1.reset_index()\n",
    "back1.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e919805-b19d-4187-a211-9d184f3c3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "back1.to_csv('back1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9706f-bb85-41b2-bf13-0a5d6be9dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttable = pd.read_csv('ttable.csv')\n",
    "assists = pd.read_csv('assists.csv')\n",
    "posses = pd.read_csv('possession.csv')\n",
    "creation = pd.read_csv('creation.csv')\n",
    "back1 = pd.read_csv('back1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25daac9d-64e0-4ed0-8fbb-e2b78b92edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season(x):\n",
    "    mainseas = ttable[ttable['Season'] == x]\n",
    "    ass = assists[assists['Season'] == x]\n",
    "    poss = posses[posses['Season'] == x]\n",
    "    cre = creation[creation['Season'] == x]\n",
    "    back = back1[back1['Season'] == x]\n",
    "\n",
    "    feat = [\"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\", 'GF', 'GA', 'xG', 'xGA', 'Poss',\n",
    "            \"KP\", \"1/3\", \"PPA\", \"CrsPA\", \"Prog\", \"Att 3rd\", \"Att Pen\", \"Prog\", 'SCA', 'TB']\n",
    "    df = pd.DataFrame()\n",
    "    df['Team'] = mainseas['Team'].unique()\n",
    "    \n",
    "    # Initialize columns with zeros\n",
    "    for i in feat:\n",
    "        df[i] = 0.0\n",
    "\n",
    "    main = [\"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\", 'GF', 'GA', 'xG', 'xGA', 'Poss']\n",
    "    asss = [\"KP\", \"1/3\", \"PPA\", \"CrsPA\", \"PrgP\"]\n",
    "    posss = [\"Att 3rd\", \"Att Pen\", \"PrgC\"]\n",
    "    gfc = ['SCA']\n",
    "    tb = ['TB']\n",
    "    feat1 = [main, asss, poss, gfc, tb]\n",
    "    tables = [mainseas, ass, poss, cre, back]\n",
    "\n",
    "\n",
    "    # Assign main features\n",
    "    for j in main:\n",
    "        for i in range(len(df)):\n",
    "            df.loc[i, j] = mainseas[mainseas['Team'] == df.loc[i, 'Team']][j].mean()\n",
    "\n",
    "    # Assign assist features\n",
    "    for j in asss:\n",
    "        for i in range(len(df)):\n",
    "            df.loc[i, j] = ass[ass['Team'] == df.loc[i, 'Team']][j].mean()\n",
    "\n",
    "    # Assign possession features\n",
    "    for j in posss:\n",
    "        for i in range(len(df)):\n",
    "            df.loc[i, j] = poss[poss['Team'] == df.loc[i, 'Team']][j].mean()\n",
    "\n",
    "    # Special feature assignments\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, 'SCA'] = cre[cre['Team'] == df.loc[i, 'Team']]['SCA'].mean()\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, 'TB'] = back[back['Team'] == df.loc[i, 'Team']]['TB'].mean()\n",
    "\n",
    "    # Add Season, Points, and Points per match columns\n",
    "    df['Season'] = x\n",
    "    df['Points'] = 0\n",
    "    df['Points per match'] = 0.0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        win_count = mainseas[mainseas['Team'] == df.loc[i, 'Team']]['Result'].value_counts().get('W', 0)\n",
    "        draw_count = mainseas[mainseas['Team'] == df.loc[i, 'Team']]['Result'].value_counts().get('D', 0)\n",
    "        df.loc[i, 'Points'] = win_count * 3 + draw_count\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, 'Points per match'] = df.loc[i, 'Points'] / len(mainseas[mainseas['Team'] == df.loc[i, 'Team']])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Creating DF for different seasons\n",
    "seas19 = season(2019)\n",
    "seas20 = season(2020)\n",
    "seas21 = season(2021)\n",
    "seas22 = season(2022)\n",
    "seas23 = season(2023)\n",
    "seas24 = season(2024)\n",
    "\n",
    "# Concatenating all the seasons\n",
    "total = pd.concat([seas19, seas20, seas21, seas22, seas23, seas24])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17043e0b-a795-4cf1-8475-14a0f09c96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.to_csv('Final_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff5e1f-9ea5-45b2-8c48-9fae7697d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a1cc7-e39e-46cb-ab5d-57bae6c41cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435b91b-0e05-4fc7-aee1-2b6252000536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url, sleep_time=10):\n",
    "    \"\"\"Fetch the URL with retries and delay.\"\"\"\n",
    "    for _ in range(3):  \n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        print(f\"Failed request with status {response.status_code}. Retrying...\")\n",
    "        time.sleep(sleep_time)\n",
    "    return None\n",
    "\n",
    "def scrape_premier_league_data():\n",
    "    standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "    years = list(range(2024, 2018, -1))\n",
    "    all_matches = []\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Fetching data for season {year}...\")\n",
    "        response = fetch_url(standings_url)\n",
    "        if not response:\n",
    "            print(f\"Skipping {year} due to repeated request failures.\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        standings_table = soup.select('table.stats_table')\n",
    "        if not standings_table:\n",
    "            print(f\"No standings table found for {year}.\")\n",
    "            continue\n",
    "\n",
    "        links = [l.get(\"href\") for l in standings_table[0].find_all('a') if l.get(\"href\") and '/squads/' in l.get(\"href\")]\n",
    "        team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "\n",
    "        \n",
    "        previous_season = soup.select(\"a.prev\")\n",
    "        if previous_season:\n",
    "            standings_url = f\"https://fbref.com{previous_season[0].get('href')}\"\n",
    "        else:\n",
    "            print(f\"No previous season link found for {year}.\")\n",
    "            break\n",
    "\n",
    "        for team_url in team_urls:\n",
    "            try:\n",
    "                team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "                team_response = fetch_url(team_url)\n",
    "                if not team_response:\n",
    "                    print(f\"Skipping {team_name} due to request failure.\")\n",
    "                    continue\n",
    "                time.sleep(10)\n",
    "                \n",
    "                matches = pd.read_html(team_response.text, match=\"Scores & Fixtures\")[0]\n",
    "                \n",
    "                soup_team = BeautifulSoup(team_response.text, \"html.parser\")\n",
    "                \n",
    "                \n",
    "                shooting_links = [l.get(\"href\") for l in soup_team.find_all('a') if l and 'all_comps/shooting/' in l]\n",
    "                if shooting_links:\n",
    "                    shooting_response = fetch_url(f\"https://fbref.com{shooting_links[0]}\")\n",
    "                    if shooting_response:\n",
    "                        shooting = pd.read_html(shooting_response.text, match=\"Shooting\")[0]\n",
    "                        shooting.columns = shooting.columns.droplevel()\n",
    "                        matches = matches.merge(shooting[['Date', 'Sh', 'SoT', 'Dist', 'FK', 'PK', 'PKatt']], on=\"Date\", how=\"left\")\n",
    "                \n",
    "                \n",
    "                passing_links = [l.get(\"href\") for l in soup_team.find_all('a') if l and 'all_comps/passing/' in l]\n",
    "                if passing_links:\n",
    "                    passing_response = fetch_url(f\"https://fbref.com{passing_links[0]}\")\n",
    "                    if passing_response:\n",
    "                        passing = pd.read_html(passing_response.text, match=\"Passing\")[0]\n",
    "                        passing.columns = passing.columns.droplevel()\n",
    "                        matches = matches.merge(passing[['Date', 'KP', '1/3', 'PPA', 'CrsPA', 'Prgp']], on=\"Date\", how=\"left\")\n",
    "                \n",
    "                \n",
    "                matches[\"Season\"] = year\n",
    "                matches[\"Team\"] = team_name\n",
    "                all_matches.append(matches)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing team {team_name} in {year}: {e}\")\n",
    "                continue\n",
    "\n",
    "    \n",
    "    final_df = pd.concat(all_matches, ignore_index=True)\n",
    "    final_df.columns = [c.lower() for c in final_df.columns]\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e894f1-c1ac-47f5-9275-ecbd2177283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('fix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29689f-f892-40d2-94ec-b9a67c0a4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttotal = pd.read_csv(r'FFinal_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b19582-260a-4549-9012-22d88b18927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_total = ttotal.drop(columns=['Prog', 'sno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cee0c7-9f28-42be-91cc-010f1bea5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_total_numeric = filtered_total.select_dtypes(include=['number'])\n",
    "\n",
    "# Adjust size dynamically based on the number of numeric columns\n",
    "num_cols = filtered_total_numeric.shape[1]\n",
    "figsize = (24, 24 * 0.6)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=figsize)\n",
    "sns.heatmap(filtered_total_numeric.corr(), annot=True, cmap='coolwarm')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc626a-6c0a-4fee-a71f-e082d623ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(r'heatmap.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd457c1b-a1b0-49de-a99b-cd29ea73e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=ttotal.drop(['Points','Season','Team','PpG'],axis=1)\n",
    "y=ttotal['Points']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79632eee-a9e0-4e0b-ac01-156e171f1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_test=seas24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66242fce-2b40-4e71-afae-620a1a9d40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seas24.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff13f3a-5366-43ad-9928-181585feb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=12)\n",
    "dt  = DecisionTreeRegressor(max_depth = 5)\n",
    "rf  = RandomForestRegressor(n_estimators=100, max_features= 7)\n",
    "ada = AdaBoostRegressor( n_estimators=150, learning_rate =.08)\n",
    "gbr = GradientBoostingRegressor(max_depth=7, n_estimators=500, learning_rate =.05)\n",
    "xgb = XGBRegressor(max_depth = 7, n_estimators=500, learning_rate =.05)\n",
    "cb  = CatBoostRegressor(learning_rate =.07, max_depth =7, verbose=0)\n",
    "\n",
    "regressors = [('K Nearest Neighbours', knn),('Decision Tree', dt), ('Random Forest', rf), \n",
    "              ('AdaBoost', ada),('Gradient Boosting Regressor', gbr),('XGBRegressor', xgb),('CatBoostRegressor', cb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027cec3-2de9-4a4c-b87e-ab335794fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for regressor_name, regressor in regressors:\n",
    "    regressor.fit(X_train, y_train)  \n",
    "    y_pred = regressor.predict(X_test)\n",
    "    accuracy = round(r2_score(y_test, y_pred), 3) * 100\n",
    "    \n",
    "    print('{:s} : {:.0f} %'.format(regressor_name, accuracy))\n",
    "    \n",
    "    \n",
    "    random_color = [random.random() for _ in range(3)]\n",
    "    plt.rcParams[\"figure.figsize\"] = (20, 8)\n",
    "    plt.bar(regressor_name, accuracy, color=random_color)\n",
    "    \n",
    "    \n",
    "    plt.text(regressor_name, accuracy + 1, f'{accuracy:.0f}%', ha='center', fontsize=12)\n",
    "\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cf7ea-a249-4f4f-959e-f49f5bce6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(r'reg_accu.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc49c9-1d5f-43b8-bc7e-661a5a0c162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_copy=seas24_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca656b-802d-4016-91f4-1ef6cd87e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seas24_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb93f0-19da-40c9-bd48-50f4ec551928",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_copy.drop(['Sh', 'SoT', 'Dist', 'FK', 'PK', 'PKatt', 'GF', 'GA', 'xG',\n",
    "       'xGA', 'Poss', 'KP', '1/3', 'PPA', 'CrsPA' , 'Att 3rd',\n",
    "       'Att Pen', 'SCA', 'TB', 'Prog'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d386ed-2bb7-4376-8ec8-34b29275dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seas24_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd43f6f-3133-4d07-80aa-d27b8c8351da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix=pd.read_csv(r'fix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880030a8-83af-4b0e-bc6d-ff41b22f9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_copy['Predicted Points XGB']=0.0\n",
    "for i in range(len(seas24_copy['Team'].unique())):\n",
    "    seas24_copy['Predicted Points XGB'][i]=seas24_copy['Points per match'][i]*(38-len(fix[(fix['Team']==seas24_copy['Team'][i])&(fix['Season']==2024)]))+seas24_copy['Points'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07bbd0-9e25-40f3-b2e6-1d8ac1605745",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_copy['Predicted Points AdaBoost']=0.0\n",
    "for i in range(len(seas24_copy['Team'].unique())):\n",
    "    seas24_copy['Predicted Points AdaBoost'][i]=seas24_copy['Points per match'][i]*(38-len(fix[(fix['Team']==seas24_copy['Team'][i])&(fix['Season']==2024)]))+seas24_copy['Points'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4148c-7860-4832-8c0f-a8e3f306c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_copy['Predicted Points Random Forest']=0.0\n",
    "for i in range(len(seas24_copy['Team'].unique())):\n",
    "    seas24_copy['Predicted Points Random Forest'][i]=seas24_copy['Points per match'][i]*(38-len(fix[(fix['Team']==seas24_copy['Team'][i])&(fix['Season']==2024)]))+seas24_copy['Points'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3d75b-2cd1-42c8-89d2-daa1f084d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_1=seas24_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe60c7-4add-4420-a8e9-ed76f391e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_1[['Predicted Points XGB','Predicted Points AdaBoost','Predicted Points Random Forest']]=seas24_1[['Predicted Points XGB','Predicted Points AdaBoost','Predicted Points Random Forest']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3419602-0acc-4353-aeed-e73b94868d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_1.sort_values('Predicted Points XGB',ascending=False)[['Team','Season','Predicted Points XGB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd004f81-8205-4731-bba1-33c7e3cbbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_1.sort_values('Predicted Points AdaBoost',ascending=False)[['Team','Season','Predicted Points AdaBoost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cfcaf1-82d4-4bfa-bc47-8088476a7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "seas24_1.sort_values('Predicted Points Random Forest',ascending=False)[['Team','Season','Predicted Points Random Forest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08be09-5365-407c-b4b9-21e4fe504353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
